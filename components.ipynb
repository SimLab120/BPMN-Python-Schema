{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b06a319",
   "metadata": {},
   "source": [
    "## Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "229db539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--140ecbb5-04d1-4514-b90d-b49a5e91e8a7-0', usage_metadata={'input_tokens': 4, 'output_tokens': 11, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b58bc",
   "metadata": {},
   "source": [
    "Chat models are language models that use a sequence of messages as inputs and return messages as outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14a67d",
   "metadata": {},
   "source": [
    "### Tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f9d6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 3.0, \"b\": 12.0}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--822008c7-ebff-4be0-8415-bfc2223d30ce-0' tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': '48fbe613-c598-427f-bf52-cf2465ef98a1', 'type': 'tool_call'}] usage_metadata={'input_tokens': 41, 'output_tokens': 5, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}}\n",
      "content='' additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 4.0}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--34a16a58-a11d-45f1-a673-218740668719-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2.0, 'b': 4.0}, 'id': '8c9f3dff-779c-47c9-a784-60b9bd9fed44', 'type': 'tool_call'}] usage_metadata={'input_tokens': 39, 'output_tokens': 5, 'total_tokens': 44, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# The function name, type hints, and docstring are all part of the tool\n",
    "# schema that's passed to the model. Defining good, descriptive schemas\n",
    "# is an extension of prompt engineering and is an important part of\n",
    "# getting models to perform well.\n",
    "\n",
    "# LangChain also implements a @tool decorator that allows for further control of the tool schema, such as tool names and argument descriptions.\n",
    "\n",
    "\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "\n",
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "print(llm_with_tools.invoke(query))\n",
    "\n",
    "llm_forced_to_multiply = model.bind_tools(tools, tool_choice=\"multiply\") # forced to use multiply\n",
    "print(llm_forced_to_multiply.invoke(\"what is 2 + 4\"))\n",
    "\n",
    "\n",
    "query = \"what is 2 + 4\"\n",
    "llm_forced_to_tool = model.bind_tools(tools, tool_choice=\"any\") # forced to use any\n",
    "result = llm_forced_to_tool.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942ff61",
   "metadata": {},
   "source": [
    "### Structured output from a chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "450020e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join the Red Cross?', punchline='They get claw-strophobic!', rating=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm = model.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51371d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_output=ConversationalResponse(response=\"Why don't scientists trust atoms? Because they make up everything!\")\n",
      "final_output=ConversationalResponse(response='I am doing great, thank you for asking! How can I help you today?')\n"
     ]
    }
   ],
   "source": [
    "from typing import Union,Literal\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ConversationalResponse(BaseModel):\n",
    "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
    "    response: str = Field(description=\"A conversational response to the user's query\")\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    final_output: Union[Joke,ConversationalResponse]\n",
    "\n",
    "\n",
    "structured_llm = model.with_structured_output(FinalResponse)\n",
    "print(structured_llm.invoke(\"Tell me a joke.\"))\n",
    "\n",
    "structured_llm = model.with_structured_output(FinalResponse)\n",
    "print(structured_llm.invoke(\"Hello, How are you today?\"))\n",
    "\n",
    "# union is not working. Not sure why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f635d",
   "metadata": {},
   "source": [
    "Not all models support .with_structured_output(). For such models you'll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output. It can be done with PydenticOutputParser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1ab0541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_output=Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!', rating=None)\n",
      "final_output=ConversationalResponse(response='I am doing well, thank you for asking! How can I help you today?')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object= FinalResponse)\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "query = \"Tell me a joke.\"\n",
    "# print(prompt.invoke({\"query\": query}))\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "print(chain.invoke({\"query\": query}))\n",
    "\n",
    "query = \"How are you?\"\n",
    "# print(prompt.invoke({\"query\": query}))\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "print(chain.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14589e0b",
   "metadata": {},
   "source": [
    "### How to cache chat model response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eae70f",
   "metadata": {},
   "source": [
    "LangChain provides an optional caching layer for chat models. This is useful for two main reasons:\n",
    "\n",
    "It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. This is especially useful during app development.\n",
    "It can also speed up your application by reducing the number of API calls you make to the LLM provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa74fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.globals import set_llm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aae8f3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalResponse(final_output=Joke(setup='Why did the cat join the Red Cross?', punchline='Because he wanted to be a first aid kit!', rating=None))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in memory cache\n",
    "from langchain_core.caches import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer, nex time onwards it is very fast\n",
    "query = \"tell me a cat joke.\"\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "50097e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalResponse(final_output=Joke(setup='Why are Dalmatians no good at hide and seek?', punchline=\"Because they're always spotted!\", rating=None))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"tell me a dog joke.\"\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8197b8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalResponse(final_output=Joke(setup='What do you call a mouse that can lift heavy things?', punchline='Muscles Mouse!', rating=7))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sqlite cache \n",
    "# This cache implementation uses a SQLite database to store responses, and will last across process restarts.\n",
    "\n",
    "from langchain_community.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "query = \"tell me a mouse joke.\"\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a55990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalResponse(final_output=Joke(setup='Why do cows wear bells?', punchline=\"Because their horns don't work!\", rating=7))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd time calling llm is super fast. just the first time its slow. look at the time of compilation. \n",
    "query = \"tell me a cow joke.\"\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9a7f3",
   "metadata": {},
   "source": [
    "### Custom chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2589455",
   "metadata": {},
   "source": [
    "Wrapping your LLM with the standard BaseChatModel interface allow you to use your LLM in existing LangChain programs with minimal code modifications!\n",
    "\n",
    "As an bonus, your LLM will automatically become a LangChain Runnable and will benefit from some optimizations out of the box (e.g., batch via a threadpool), async support, the astream_events API, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb119c8",
   "metadata": {},
   "source": [
    "#### Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72113666",
   "metadata": {},
   "source": [
    "messages are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n",
    "\n",
    "Each message has a role (e.g., \"user\", \"assistant\") , content (e.g., text, multimodal data), additional metadata(id, name, token usage and other model-specific metadata).\n",
    "\n",
    "LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c30d8d",
   "metadata": {},
   "source": [
    "**roles:**\n",
    "\n",
    "Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\n",
    "\n",
    "| **Role**              | **Description**                                                                                                                                                                                                 |\n",
    "|-----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **system**            | Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.                                                                                            |\n",
    "| **user**              | Represents input from a user interacting with the model, usually in the form of text or other interactive input.                                                                                                |\n",
    "| **assistant**         | Represents a response from the model, which can include text or a request to invoke tools.                                                                                                                      |\n",
    "| **tool**              | A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support [tool calling](/docs/concepts/tool_calling). |\n",
    "| **function** (legacy) | This is a legacy role, corresponding to OpenAI's legacy function-calling API. **tool** role should be used instead.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Content:**\n",
    "\n",
    "Currently, most chat models support text as the primary content type, with some models also supporting multimodal data.\n",
    "\n",
    "\n",
    "Other Message Data: \n",
    "\n",
    "Depending on the chat model provider, messages can include other data such as\n",
    "\n",
    "ID: An optional unique identifier for the message.\n",
    "\n",
    "Name: An optional name property which allows differentiate between different entities/speakers with the same role. Not all models support this!\n",
    "\n",
    "**Metadata:** Additional information about the message, such as timestamps, token usage, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d617703",
   "metadata": {},
   "source": [
    "4 Important types of messages:\n",
    "1. SystemMessage -- for content which should be passed to direct the conversation, corresponds to system role. instructing the model to adopt a specific persona or setting the tone of the conversation. Different chat providers may support system message in one of the following ways:\n",
    "\n",
    "    Through a \"system\" message role: In this case, a system message is included as part of the message sequence with the role explicitly set as \"system.\"\n",
    "\n",
    "    Through a separate API parameter for system instructions: Instead of being included as a message, system instructions are passed via a dedicated API parameter.\n",
    "\n",
    "    No support for system messages: Some models do not support system messages at all.\n",
    "\n",
    "    LangChain will automatically adapt based on the provider’s capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.\n",
    "\n",
    "2. HumanMessage -- for content in the input from the user. corresponds to user role\n",
    "\n",
    "3. AIMessage -- for content in the response from the model. corresponds to assistant role. This is the response from the model, which can include text or a request to invoke tools. An AIMessage has the following attributes: content, tool_call, invalid_tool_calls, usage_metadata, id, response_metadata. standardized attributes(tool_call, invalid_tool_calls, usage_metadata, id)are the ones that LangChain attempts to standardize across different chat model providers. raw fields (content,response_metadata)are specific to the model provider and may vary.\n",
    "\n",
    "4. ToolMessage -- corresponds to tool role. In addition to role and content, this message has: a tool_call_id field which conveys the id of the call to the tool that was called to produce this result, an artifact field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2c3c7",
   "metadata": {},
   "source": [
    "Conversation Structure:\n",
    "\n",
    "The sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.\n",
    "\n",
    "For example, a typical conversation structure might look like this:\n",
    "\n",
    "User Message: \"Hello, how are you?\"\n",
    "\n",
    "Assistant Message: \"I'm doing well, thank you for asking.\"\n",
    "\n",
    "User Message: \"Can you tell me a joke?\"\n",
    "\n",
    "Assistant Message: \"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
    "\n",
    "\n",
    "Most conversations start with a system message that sets the context for the conversation. This is followed by a user message containing the user's input, and then an assistant message containing the model's response.\n",
    "The assistant may respond directly to the user or if configured with tools request that a tool be invoked to perform a specific task.\n",
    "\n",
    "\n",
    "A full conversation often involves a combination of two patterns of alternating messages:\n",
    "\n",
    "The user and the assistant representing a back-and-forth conversation.\n",
    "The assistant and tool messages representing an \"agentic\" workflow where the assistant is invoking tools to perform specific tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6e1a3",
   "metadata": {},
   "source": [
    "Managing chat history:\n",
    "\n",
    "Since chat models have a maximum limit on input size, it's important to manage chat history and trim it as needed to avoid exceeding the context window.\n",
    "\n",
    "While processing chat history, it's essential to preserve a correct conversation structure.\n",
    "\n",
    "Key guidelines for managing chat history:\n",
    "\n",
    "The conversation should follow one of these structures:\n",
    "The first message is either a \"user\" message or a \"system\" message, followed by a \"user\" and then an \"assistant\" message.\n",
    "The last message should be either a \"user\" message or a \"tool\" message containing the result of a tool call.\n",
    "When using tool calling, a \"tool\" message should only follow an \"assistant\" message that requested the tool invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d23e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9833bd",
   "metadata": {},
   "source": [
    "#### Streaming Variant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c520e3",
   "metadata": {},
   "source": [
    "All the chat messages have a streaming variant that contains Chunk in the name.\n",
    "\n",
    "These chunks are used when streaming output from chat models, and they all define an additive property!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2016086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessageChunk,\n",
    "    FunctionMessageChunk,\n",
    "    HumanMessageChunk,\n",
    "    SystemMessageChunk,\n",
    "    ToolMessageChunk,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f337b0",
   "metadata": {},
   "source": [
    "#### BaseChatModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f5c701",
   "metadata": {},
   "source": [
    " we will inherit from `BaseChatModel` and we'll need to implement the following:\n",
    "\n",
    " | Method/Property                | Description                                                        | Required/Optional |\n",
    "|--------------------------------|--------------------------------------------------------------------|-------------------|\n",
    "| `_generate`                    | Use to generate a chat result from a prompt                        | Required          |\n",
    "| `_llm_type` (property)         | Used to uniquely identify the type of the model. Used for logging. | Required          |\n",
    "| `_identifying_params` (property)| Represent model parameterization for tracing purposes.             | Optional          |\n",
    "| `_stream`                      | Use to implement streaming.                                        | Optional          |\n",
    "| `_agenerate`                   | Use to implement a native async method.                            | Optional          |\n",
    "| `_astream`                     | Use to implement async version of `_stream`.                       | Optional          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8a872",
   "metadata": {},
   "source": [
    "The _astream implementation uses run_in_executor to launch the sync _stream in a separate thread if _stream is implemented, otherwise it fallsback to use _agenerate.\n",
    "\n",
    "You can use this trick if you want to reuse the _stream implementation, but if you're able to implement code that's natively async that's a better solution since that code will run with less overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f27e6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    ")\n",
    "from langchain_core.messages.ai import UsageMetadata\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class ChatParrotLink(BaseChatModel):\n",
    "    \"\"\"A custom chat model that echoes the first `parrot_buffer_length` characters\n",
    "    of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = ChatParrotLink(parrot_buffer_length=2, model=\"bird-brain-001\")\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = Field(alias=\"model\")\n",
    "    \"\"\"The name of the model\"\"\"\n",
    "    parrot_buffer_length: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "    temperature: Optional[float] = None\n",
    "    max_tokens: Optional[int] = None\n",
    "    timeout: Optional[int] = None\n",
    "    stop: Optional[List[str]] = None\n",
    "    max_retries: int = 2\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Override the _generate method to implement the chat model logic.\n",
    "\n",
    "        This can be a call to an API, a call to a local model, or any other\n",
    "        implementation that generates a response to the input prompt.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        # Replace this with actual logic to generate a response from a list\n",
    "        # of messages.\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[: self.parrot_buffer_length]\n",
    "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
    "        ct_output_tokens = len(tokens)\n",
    "        message = AIMessage(\n",
    "            content=tokens,\n",
    "            additional_kwargs={},  # Used to add additional payload to the message\n",
    "            response_metadata={  # Use for response metadata\n",
    "                \"time_in_seconds\": 3,\n",
    "                \"model_name\": self.model_name,\n",
    "            },\n",
    "            usage_metadata={\n",
    "                \"input_tokens\": ct_input_tokens,\n",
    "                \"output_tokens\": ct_output_tokens,\n",
    "                \"total_tokens\": ct_input_tokens + ct_output_tokens,\n",
    "            },\n",
    "        )\n",
    "        ##\n",
    "\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"Stream the output of the model.\n",
    "\n",
    "        This method should be implemented if the model can generate output\n",
    "        in a streaming fashion. If the model does not support streaming,\n",
    "        do not implement it. In that case streaming requests will be automatically\n",
    "        handled by the _generate method.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        last_message = messages[-1]\n",
    "        tokens = str(last_message.content[: self.parrot_buffer_length])\n",
    "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
    "\n",
    "        for token in tokens:\n",
    "            usage_metadata = UsageMetadata(\n",
    "                {\n",
    "                    \"input_tokens\": ct_input_tokens,\n",
    "                    \"output_tokens\": 1,\n",
    "                    \"total_tokens\": ct_input_tokens + 1,\n",
    "                }\n",
    "            )\n",
    "            ct_input_tokens = 0\n",
    "            chunk = ChatGenerationChunk(\n",
    "                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)\n",
    "            )\n",
    "\n",
    "            if run_manager:\n",
    "                # This is optional in newer versions of LangChain\n",
    "                # The on_llm_new_token will be called automatically\n",
    "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "        # Let's add some other information (e.g., response metadata)\n",
    "        chunk = ChatGenerationChunk(\n",
    "            message=AIMessageChunk(\n",
    "                content=\"\",\n",
    "                response_metadata={\"time_in_sec\": 3, \"model_name\": self.model_name},\n",
    "            )\n",
    "        )\n",
    "        if run_manager:\n",
    "            # This is optional in newer versions of LangChain\n",
    "            # The on_llm_new_token will be called automatically\n",
    "            run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "        yield chunk\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"echoing-chat-model-advanced\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        This information is used by the LangChain callback system, which\n",
    "        is used for tracing purposes make it possible to monitor LLMs.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": self.model_name,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bae972",
   "metadata": {},
   "source": [
    "### How to stream chat model response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3fbc8",
   "metadata": {},
   "source": [
    "The stream method is a way to retrieve output from a language/chat model in a piece-by-piece manner, rather than waiting for the entire response at once. It provides an iterator that yields data as it becomes available.\n",
    "\n",
    "Why is stream useful ?\n",
    "1. Responsiveness: Users can see the response as it’s being generated, improving the interactive experience (like watching words appear in a chat window).\n",
    "2. Early Processing: Applications can start processing or displaying information before the model has finished its entire response.\n",
    "3. Token-by-token control: If supported by the model provider, you can handle every word or token as it’s produced.\n",
    "\n",
    "\n",
    "Behaviours : \n",
    "\n",
    "1. Default behavior (no real streaming, stream method not defined in custom model):\n",
    "    The default implementation of stream yields a single value: the final output.\n",
    "    Even though you call stream, you don’t get intermediate results—instead, you get the complete response at the end, just as if you had called a regular invoke method.\n",
    "    This ensures API compatibility for all chat models, even those that don’t support real streaming.\n",
    "    ```\n",
    "    for chunk in model.stream(\"Tell me a joke.\"):\n",
    "        print(chunk)\n",
    "    # Output (after a pause):\n",
    "    # Why did the chicken cross the road? To get to the other side!\n",
    "    ```\n",
    "\n",
    "2. With provider streaming (real streaming, stream method defined in custom model method):\n",
    "If the chat model provider supports streaming, the stream method will yield multiple chunks as the model generates them (often token-by-token or sentence-by-sentence depending upon the implementation of stream method).\n",
    "This means your app can display new content to the user as soon as it’s ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad63aec",
   "metadata": {},
   "source": [
    "All chat models implement the Runnable interface, which comes with a default implementations of standard runnable methods (i.e. invoke, ainvoke, batch, abatch, stream, astream, astream_events)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ccaf73",
   "metadata": {},
   "source": [
    "#### Sync vs Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8c4b0",
   "metadata": {},
   "source": [
    "stream: Synchronous Streaming\n",
    "\n",
    "How it works:\n",
    "You use a regular (blocking) for loop to iterate over the streamed output.\n",
    "\n",
    "When to use:\n",
    "In standard Python scripts or environments where asynchronous code (async/await) is not needed.\n",
    "\n",
    "\n",
    "```\n",
    "from langchain_anthropic.chat_models import ChatAnthropic\n",
    "\n",
    "chat = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "for chunk in chat.stream(\"Write me a 1 verse song about goldfish on the moon\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "```\n",
    "\n",
    "Here, each chunk.content is printed as soon as it is available.\n",
    "\n",
    "This code blocks execution until each new chunk is received and printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebc9b9",
   "metadata": {},
   "source": [
    "astream: Asynchronous Streaming\n",
    "\n",
    "How it works:\n",
    "You use an async for loop, which allows other tasks to run while waiting for new pieces of the response.\n",
    "\n",
    "When to use:\n",
    "In asynchronous Python applications (using asyncio), such as web servers (FastAPI, etc.), GUIs, or whenever you want non-blocking behavior.\n",
    "\n",
    "```\n",
    "from langchain_anthropic.chat_models import ChatAnthropic\n",
    "\n",
    "chat = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "async for chunk in chat.astream(\"Write me a 1 verse song about goldfish on the moon\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "```\n",
    "\n",
    "As soon as each piece (chunk) of text is ready, it’s printed out immediately.\n",
    "If the model streams token by token, you’ll see the answer appear gradually, like typing but not with astream. \n",
    "\n",
    "Here, the output is streamed asynchronously, so your program can perform other operations(other operations in the for loop) while waiting for each chunk.\n",
    "\n",
    "This requires your code to be inside an async function and run within an event loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a4047",
   "metadata": {},
   "source": [
    "astream_events:\n",
    "\n",
    "\n",
    "Purpose:\n",
    "It streams not only the output chunks (tokens, messages, etc.) but also detailed events that occur during the execution of a runnable/chat model.\n",
    "\n",
    "Usage:\n",
    "You use it with an async for loop, just like astream, but instead of only getting content chunks, you receive event dictionaries (with event types, metadata, etc.).\n",
    "\n",
    "\n",
    "```\n",
    "async for event in chat.astream_events(\"Write me a 1 verse song about goldfish on the moon\"):\n",
    "    print(event)\n",
    "```\n",
    "Typical events include:\n",
    "\n",
    "on_chat_model_start\n",
    "\n",
    "on_chat_model_stream (one for each chunk/token)\n",
    "\n",
    "on_chat_model_end\n",
    "\n",
    "...and more, with accompanying metadata\n",
    "\n",
    "\n",
    "\n",
    "Use astream_events if you want:\n",
    "\n",
    "1. To monitor the full lifecycle of an LLM call (start, each chunk, end, errors, etc.)\n",
    "2. To build complex, interactive UIs or logs that react to more than just the model's text output\n",
    "3. To integrate with pipelines/chains where you need to track progress, errors, or intermediate steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1689b",
   "metadata": {},
   "source": [
    "### Response Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3c3ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
       " 'finish_reason': 'STOP',\n",
       " 'safety_ratings': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBuTnZ2CpdF3bfZsuyomwWJqbQOIf3hU8o\"\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "msg = llm.invoke(\"What's the oldest known example of cuneiform\")\n",
    "msg.response_metadata\n",
    "\n",
    "# different chat model providers have different response_metadata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1ac10",
   "metadata": {},
   "source": [
    "### How to use chat models to call tools "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6a3f6",
   "metadata": {},
   "source": [
    "The chat model can only generate the arguments to a tool, and actually running the tool (or not) is up to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c65bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--23f03b53-7f47-4189-ab4d-879c2b3787fa-0', usage_metadata={'input_tokens': 4, 'output_tokens': 11, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBuTnZ2CpdF3bfZsuyomwWJqbQOIf3hU8o\"\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3488ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "llm_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98fb42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 3.0, \"b\": 12.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--fe56a27c-5430-48b3-b1d5-e202acb0f4e9-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': '24ac9e80-cf73-4c08-bb6b-5053f25c0c37', 'type': 'tool_call'}], usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = model.bind_tools(tools)\n",
    "\n",
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "llm_with_tools.invoke(query)\n",
    "\n",
    "# llm generates arugment to a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c98195e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"a\": 11.0, \"b\": 49.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--b4e81bbc-1749-40c8-8d62-0cbcbf3a95b2-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': 'b11eed05-eef1-4e4d-9038-656a73c58184', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11.0, 'b': 49.0}, 'id': 'a87f9b79-f82c-4822-a044-b9ed1b77299c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 75, 'output_tokens': 10, 'total_tokens': 85, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that chat models can call multiple tools at once.\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffc29a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe .tool_calls attribute should contain valid tool calls. \\nNote that on occasion, model providers may output malformed tool calls (e.g., arguments that are not valid JSON).\\n When parsing fails in these cases, instances of InvalidToolCall are populated in the .invalid_tool_calls attribute. \\n An InvalidToolCall can have a name, string arguments, identifier, and error message.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call multiple tools at once\n",
    "llm_with_tools.invoke(query).tool_calls\n",
    "\n",
    "'''\n",
    "The .tool_calls attribute should contain valid tool calls. \n",
    "Note that on occasion, model providers may output malformed tool calls (e.g., arguments that are not valid JSON).\n",
    " When parsing fails in these cases, instances of InvalidToolCall are populated in the .invalid_tool_calls attribute. \n",
    " An InvalidToolCall can have a name, string arguments, identifier, and error message.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "280448e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(query).invalid_tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38b681",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StructuredTool' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat is 3 * 12? Also, what is 11 + 49?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m chain = llm_with_tools | PydanticToolsParser(tools=[add, multiply])\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# chain.invoke(query)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pragbehera\\.virtualenvs\\learn_langchain_2-zo9tOhzO\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pragbehera\\.virtualenvs\\learn_langchain_2-zo9tOhzO\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:196\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pragbehera\\.virtualenvs\\learn_langchain_2-zo9tOhzO\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pragbehera\\.virtualenvs\\learn_langchain_2-zo9tOhzO\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pragbehera\\.virtualenvs\\learn_langchain_2-zo9tOhzO\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:197\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    200\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    201\u001b[39m             config,\n\u001b[32m    202\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m         )\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pragbehera\\.virtualenvs\\learn_langchain_2-zo9tOhzO\\Lib\\site-packages\\langchain_core\\output_parsers\\openai_tools.py:294\u001b[39m, in \u001b[36mPydanticToolsParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.first_tool_only \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    293\u001b[39m json_results = [json_results] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.first_tool_only \u001b[38;5;28;01melse\u001b[39;00m json_results\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m name_dict = {\u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m: tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tools}\n\u001b[32m    295\u001b[39m pydantic_objects = []\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m json_results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pragbehera\\.virtualenvs\\learn_langchain_2-zo9tOhzO\\Lib\\site-packages\\pydantic\\main.py:988\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m, item):\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m         \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m    991\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'StructuredTool' object has no attribute '__name__'"
     ]
    }
   ],
   "source": [
    "# parsing \n",
    "from langchain_core.output_parsers import PydanticToolsParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "chain = llm_with_tools | PydanticToolsParser(tools=[add, multiply])\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43a5b9b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HumanMessage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# actually using the tool by invoking the function and passing the results back to the model\u001b[39;00m\n\u001b[32m      3\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat is 3 * 12? Also, what is 11 + 49?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m messages = [\u001b[43mHumanMessage\u001b[49m(query)]\n\u001b[32m      7\u001b[39m tool_dict = {\u001b[33m\"\u001b[39m\u001b[33madd\u001b[39m\u001b[33m\"\u001b[39m: add, \u001b[33m\"\u001b[39m\u001b[33mmultiply\u001b[39m\u001b[33m\"\u001b[39m: multiply}\n\u001b[32m      9\u001b[39m ai_msg = llm_with_tools.invoke(query)\n",
      "\u001b[31mNameError\u001b[39m: name 'HumanMessage' is not defined"
     ]
    }
   ],
   "source": [
    "# actually using the tool by invoking the function and passing the results back to the model\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "tool_dict = {\"add\": add, \"multiply\": multiply}\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = tool_dict[tool_call[\"name\"].lower()]\n",
    "    print(selected_tool)\n",
    "    # tool_msg = selected_tool.invoke(tool_call)\n",
    "    # messages.append(tool_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "345ee1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply',\n",
       " 'args': {'a': 3.0, 'b': 12.0},\n",
       " 'id': '3d032434-b9b6-4915-bd21-5c6aadbb416d',\n",
       " 'type': 'tool_call'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eeb438",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0937a8a",
   "metadata": {},
   "source": [
    "A vector store stores embedded data and performs similarity search.\n",
    "\n",
    "\"In-memory\" vector stores in LangChain (and similar frameworks) are simple vector databases that keep all data (vectors and documents) directly in your computer’s memory (RAM), rather than saving them to disk, a cloud service, or a dedicated database.\n",
    "\n",
    "\n",
    "Other vector store integrated with langchain - astradb, chroma, faiss, milvus, mangodb, pinecone,  pgvector, qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBvRvc3NjHH3B1_3DJO1dKiRcEJR4G64A8\"\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "# Embedding models create a vector representation of a piece of text.\n",
    "\n",
    "# in memory \n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# astradb\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "vector_store = AstraDBVectorStore(\n",
    "    embedding=embeddings,\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "    collection_name=\"astra_vector_langchain\",\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    namespace=ASTRA_DB_NAMESPACE,\n",
    ")\n",
    "\n",
    "# chroma\n",
    "from langchain_chroma import Chroma\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "# pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=...)\n",
    "index = pc.Index(index_name)\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa69170",
   "metadata": {},
   "source": [
    "## Dcoument Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c5e66",
   "metadata": {},
   "source": [
    "DocumentLoaders load data into the standard LangChain Document format.\n",
    "Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method. \n",
    "\n",
    "It can load below with multiple loader\n",
    "\n",
    "1. webpages\n",
    "2. pdf\n",
    "3. cloud provider\n",
    "4. social platform(twitter,reddit)\n",
    "5. messaging services\n",
    "6. Productivity tool(github,slack)\n",
    "7. csv loader, directory loader, json loader, bshtml loader , unstructured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(\n",
    "    ...  # <-- Integration specific parameters here\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8850f",
   "metadata": {},
   "source": [
    "vector store classes do implement a class method called from_documents (or similar) for data ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf1224",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f9b80",
   "metadata": {},
   "source": [
    "A Retriever is an object in LangChain that finds and returns relevant documents from a collection, given a user query.\n",
    "Think of it like a search engine: you give it a question or some text, and it returns the most relevant pieces of information. A retriever is a high-level abstraction for fetching relevant documents \n",
    "\n",
    "Purpose: Abstracts the way documents are fetched, so you can swap out different retrieval methods (like keyword search, vector similarity, hybrid, etc.).\n",
    "Interface: Typically, it has a .get_relevant_documents(query) method. It Can use vector store internally.\n",
    "\n",
    "Retrievers can be created from vector stores. and  all vector stores can be cast to retrievers. Though vector store might have some search method but retriever can also use other methods (keyword search, hybrid search, filtering, etc.). It provides a consistent interface (get_relevant_documents(query)) regardless of the retrieval method used.\n",
    "\n",
    "Retrievers accept a string query as input and return a list of Documents as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe1953",
   "metadata": {},
   "source": [
    "### How to use vector store as retriever ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735a822",
   "metadata": {},
   "source": [
    "You can build a retriever from a vectorstore using its .as_retriever method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"state_of_the_union.txt\")\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"score_threshold\": 0.5,\"k\": 1})\n",
    "docs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n",
    "\n",
    "# search_type = similarity_search, max_marginal_relevance_search,etc\n",
    "# Similarity score threshold retrieval by score_threshold\n",
    "# limit the number of document with parameter k - top k doc retrieval \n",
    "\n",
    "# different vector stores will have different \"??_kwargs\" arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39560e03",
   "metadata": {},
   "source": [
    "### MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a718321",
   "metadata": {},
   "source": [
    "Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on a distance metric. But, retrieval may produce different results with subtle changes in query wording, or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\n",
    "\n",
    "The MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever can mitigate some of the limitations of the distance-based retrieval and get a richer set of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b062d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ") # instead of llm , you can provide llm_chain also\n",
    "\n",
    "unique_docs = retriever_from_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ba906",
   "metadata": {},
   "source": [
    "### Contextual compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1db94",
   "metadata": {},
   "source": [
    "Sometimes the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
    "\n",
    "\n",
    "To use the Contextual Compression Retriever, you'll need:\n",
    "\n",
    "1. a base retriever\n",
    "2. a Document Compressor\n",
    "\n",
    "The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fcb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor # there are more built in compressors\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# base retriever\n",
    "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
    "\n",
    "# compressor\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# compressed retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaining multiple compressors and document transform togather \n",
    "\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \") # first compressor\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings) # 2nd compressor # Remove redundant (duplicate or near-duplicate) documents\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76) # 3rd compressor # Select only documents similar to a reference/query embedding\n",
    "\n",
    "# combine all compressors \n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")\n",
    "\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f481e",
   "metadata": {},
   "source": [
    "### custom Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac81719",
   "metadata": {},
   "source": [
    "To create your own retriever, you need to extend the BaseRetriever class and implement the following methods:\n",
    "1. _get_relevant_documents\t\n",
    "2. _aget_relevant_documents\t(optional)\n",
    "\n",
    "By inherting from BaseRetriever, your retriever automatically becomes a LangChain Runnable and will gain the standard Runnable functionality out of the box!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "class ToyRetriever(BaseRetriever):\n",
    "    \"\"\"A toy retriever that contains the top k documents that contain the user query.\n",
    "\n",
    "    This retriever only implements the sync method _get_relevant_documents.\n",
    "\n",
    "    If the retriever were to involve file access or network access, it could benefit\n",
    "    from a native async implementation of `_aget_relevant_documents`.\n",
    "\n",
    "    As usual, with Runnables, there's a default async implementation that's provided\n",
    "    that delegates to the sync implementation running on another thread.\n",
    "    \"\"\"\n",
    "\n",
    "    documents: List[Document]\n",
    "    \"\"\"List of documents to retrieve from.\"\"\"\n",
    "    k: int\n",
    "    \"\"\"Number of top results to return\"\"\"\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "        matching_documents = []\n",
    "        for document in self.documents:\n",
    "            if len(matching_documents) > self.k:\n",
    "                return matching_documents\n",
    "\n",
    "            if query.lower() in document.page_content.lower():\n",
    "                matching_documents.append(document)\n",
    "        return matching_documents\n",
    "\n",
    "    # Optional: Provide a more efficient native implementation by overriding\n",
    "    # _aget_relevant_documents\n",
    "    # async def _aget_relevant_documents(\n",
    "    #     self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\n",
    "    # ) -> List[Document]:\n",
    "    #     \"\"\"Asynchronously get documents relevant to a query.\n",
    "\n",
    "    #     Args:\n",
    "    #         query: String to find relevant documents for\n",
    "    #         run_manager: The callbacks handler to use\n",
    "\n",
    "    #     Returns:\n",
    "    #         List of relevant documents\n",
    "    #     \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb49d1",
   "metadata": {},
   "source": [
    "### How to add score to retriever result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64026860",
   "metadata": {},
   "source": [
    "how to add retrieval scores to the .metadata of documents from vector store retriever and from higher order LangChain retrievers, such as SelfQueryRetriever or MultiVectorRetriever.\n",
    "\n",
    "For vector store retriever ,  we will implement a short wrapper function around the corresponding vector store. \n",
    "\n",
    "For higher order langchain retriever, we will update a method of the corresponding class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da990aa5",
   "metadata": {},
   "source": [
    "To obtain scores from a vector store retriever, we wrap the underlying vector store's .similarity_search_with_score method in a short function that packages scores into the associated document's metadata.\n",
    "\n",
    "\n",
    "Steps:\n",
    "- A custom retriever function is defined, decorated with `@chain` to make it a Runnable (usable in LangChain pipelines).\n",
    "- It calls `vectorstore.similarity_search_with_score(query)`, which returns pairs of (`Document`, `score`).\n",
    "- The score is inserted into each document's `.metadata`.\n",
    "- The function returns the list of enriched `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ef223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    docs, scores = zip(*vectorstore.similarity_search_with_score(query))\n",
    "    for doc, score in zip(docs, scores):\n",
    "        doc.metadata[\"score\"] = score\n",
    "\n",
    "    return docs\n",
    "\n",
    "result = retriever.invoke(\"dinosaur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f66f74",
   "metadata": {},
   "source": [
    "#### SelfQueryRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af7a345",
   "metadata": {},
   "source": [
    "SelfQueryRetriever is a class that  will use a LLM to generate a query that is potentially structured.\n",
    "It can generate structured queries that go beyond simple text search—like filtering documents based on metadata (attributes like year, genre, or rating).\n",
    "\n",
    "\n",
    "SelfQueryRetriever includes a short (1 - 2 line) method _get_docs_with_query that executes the vectorstore search. We can subclass SelfQueryRetriever and override this method to propagate similarity scores.\n",
    "\n",
    "**SelfQueryRetriever** uses an LLM to build more complex (structured) queries, possibly including filters on metadata.\n",
    "\n",
    "Steps:\n",
    "- **The self-query retriever requires you to have lark package installed.**\n",
    "- Set up attribute metadata (e.g., `genre`, `year`, etc.) for filtering.\n",
    "- Instantiate a language model (`ChatOpenAI`).\n",
    "- Subclass `SelfQueryRetriever` and override its `_get_docs_with_query` method:\n",
    "    - Use `vectorstore.similarity_search_with_score` to get both docs and scores.\n",
    "    - Insert scores into each document's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# defining the structure of metadata field on your document. \n",
    "# tell the LLM what fields are available for filtering and how to use them.\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\" # A short string describing what the main content of each document \n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb96eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "\n",
    "class CustomSelfQueryRetriever(SelfQueryRetriever):\n",
    "    def _get_docs_with_query(self, query: str, search_kwargs: Dict[str, Any]) -> List[Document]:\n",
    "        \"\"\"Get docs, adding score information.\"\"\"\n",
    "        docs, scores = zip(\n",
    "            *self.vectorstore.similarity_search_with_score(query, **search_kwargs)\n",
    "        )\n",
    "        for doc, score in zip(docs, scores):\n",
    "            doc.metadata[\"score\"] = score\n",
    "\n",
    "        return docs\n",
    "    \n",
    "\n",
    "retriever = CustomSelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    enable_limit=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = retriever.invoke(\"dinosaur movie with rating less than 8\")\n",
    "\n",
    "\n",
    "# under the hood:\n",
    "# the retriever constructs a prompt for llm using user's query, document_content_description, metadata_field_info (with allowed filters/fields).\n",
    "'''\n",
    "Prompt ex: \n",
    "User wants to find: dinosaur movie with rating less than 8\n",
    "Each document is: Brief summary of a movie\n",
    "You can filter on:\n",
    "  - genre: string, \"The genre of the movie...\"\n",
    "  - year: integer, \"The year the movie was released\"\n",
    "  - rating: float, \"A 1-10 rating for the movie\"\n",
    "''' \n",
    "# llm receives the prompt and interprets the query and output a structured query\n",
    "'''\n",
    "prompt ex: \n",
    "Text search: \"dinosaur\"\n",
    "Metadata filter: rating < 8\n",
    "'''\n",
    "\n",
    "# The retriever takes the structured query: Runs a semantic search in the vectorstore using the text (\"dinosaur\") ,Applies the metadata filter (rating < 8)\n",
    "# The vectorstore executes a filtered similarity search and returns the top-matching documents with scores added \n",
    "# final output : The original content(Brief summary of a movie) ,The original metadata, The added \"score\" in metadata.\n",
    "# both the filterable fields and the similarity score are stored in the same metadata dictionary of each document.\n",
    "'''\n",
    "(\n",
    "    Document(\n",
    "        page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose',\n",
    "        metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993.0, 'score': 0.84429127}\n",
    "    ),\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "When you use SelfQueryRetriever with enable_limit=True,\n",
    " you do not pass k as a separate parameter in your code. \n",
    " Instead, you specify the desired number of results in your natural language query.\n",
    "   The retriever uses an LLM to parse your query, extract the limit, and apply it.\n",
    "'''\n",
    "results = retriever.invoke(\"Give me two movies about science fiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa30dd",
   "metadata": {},
   "source": [
    "#### MultiVectorRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3906e",
   "metadata": {},
   "source": [
    "MultiVectorRetriever is a special retriever in LangChain that allows each logical document to be associated with multiple vectors (embeddings).\n",
    "This is different from the traditional retriever where one document = one vector.\n",
    "\n",
    "\n",
    "Sometimes, a single document is too large or too complex to be represented by a single embedding vector.\n",
    "\n",
    "Solution:\n",
    "Break the document into meaningful chunks (sub-documents), embed each chunk separately, and store them all in the vectorstore.\n",
    "Each chunk is linked to the parent document via a unique ID.\n",
    "\n",
    "\n",
    "Benefits:\n",
    "More accurate retrieval (retrieves based on relevant chunk, but presents the whole document)\n",
    "Supports highlighting or surfacing why a document was retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1bace",
   "metadata": {},
   "source": [
    "Chunk and Embed:\n",
    "Split each large document into smaller sub-documents (chunks).\n",
    "Each chunk is embedded and stored in the vectorstore, with a metadata field like \"doc_id\": \"parent_doc_id\".\n",
    "\n",
    "Store Parents:\n",
    "Store the full parent documents in a docstore (can be in-memory or persistent).\n",
    "\n",
    "Query:\n",
    "At retrieval time, the user query is embedded, and the vectorstore is searched among all sub-documents(chunks).\n",
    "The retriever finds the most relevant chunks.\n",
    "For each relevant chunk, the retriever fetches its parent document using doc_id.\n",
    "Optionally, the retriever can include the actual sub-documents (and their scores) in the parent document’s metadata to show why the parent was retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1664b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The storage layer for the parent documents\n",
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "docstore = InMemoryStore()\n",
    "fake_whole_documents = [\n",
    "    (\"fake_id_1\", Document(page_content=\"fake whole document 1\")),\n",
    "    (\"fake_id_2\", Document(page_content=\"fake whole document 2\")),\n",
    "]\n",
    "docstore.mset(fake_whole_documents)  # can store in a vector store also\n",
    "# docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897797e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metadata so each chunk knows its parent \n",
    "sub_docs = [\n",
    "    Document(\n",
    "        page_content=\"A snippet from a larger document discussing cats.\",\n",
    "        metadata={\"doc_id\": \"fake_id_1\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A snippet from a larger document discussing discourse.\",\n",
    "        metadata={\"doc_id\": \"fake_id_1\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A snippet from a larger document discussing chocolate.\",\n",
    "        metadata={\"doc_id\": \"fake_id_2\"},\n",
    "    ),\n",
    "]\n",
    "vectorstore.add_documents(sub_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7078ae",
   "metadata": {},
   "source": [
    "To propagate the scores, we subclass MultiVectorRetriever and override its _get_relevant_documents method. Here we will make two changes:\n",
    "\n",
    "We will add similarity scores to the metadata of the corresponding \"sub-documents\" using the similarity_search_with_score method of the underlying vector store as above;\n",
    "We will include a list of these sub-documents in the metadata of the retrieved parent document. This surfaces what snippets of text were identified by the retrieval, together with their corresponding similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "\n",
    "\n",
    "class CustomMultiVectorRetriever(MultiVectorRetriever):\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "        \"\"\"Get documents relevant to a query.\n",
    "        Args:\n",
    "            query: String to find relevant documents for\n",
    "            run_manager: The callbacks handler to use\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        results = self.vectorstore.similarity_search_with_score(\n",
    "            query, **self.search_kwargs\n",
    "        )\n",
    "\n",
    "        #docs, scores = zip(*self.vectorstore.similarity_search_with_score(query, **search_kwargs))\n",
    "\n",
    "        # Map doc_ids to list of sub-documents, adding scores to metadata\n",
    "        id_to_doc = defaultdict(list) # key - doc_id , value - list of sub_dcos\n",
    "        for sub_doc, score in results:\n",
    "            sub_doc.metadata[\"score\"] = score\n",
    "            doc_id = sub_doc.metadata.get(\"doc_id\")\n",
    "            if doc_id:\n",
    "                id_to_doc[doc_id].append(sub_doc)\n",
    "\n",
    "        # Fetch documents corresponding to doc_ids, retaining sub_docs in metadata\n",
    "        docs = []\n",
    "        for _id, sub_docs in id_to_doc.items(): # Iterate through the dictionary id_to_doc, _id is the parent document ID,\n",
    "            # sub_docs is a list of sub-documents (chunks) from the vectorstore that matched the query and are linked to this parent.\n",
    "\n",
    "            docstore_docs = self.docstore.mget([_id]) # Retrieve the parent document(s) from the docstore (could be a database, in-memory store, etc.) using the parent ID.\n",
    "            # mget([_id]) returns a list (even if only one doc matches).\n",
    "\n",
    "            if docstore_docs: # Check if any parent documents were found for this ID (the list is not empty).\n",
    "                if doc := docstore_docs[0]: # check if the first (and usually only) parent document in the list is None or not \n",
    "                    doc.metadata[\"sub_docs\"] = sub_docs # Attach the list of matching sub-documents (chunks) to the parent document’s metadata under the key \"sub_docs\".\n",
    "                    docs.append(doc) # Add the parent document (now with its relevant sub-documents in its metadata) to the results list.\n",
    "\n",
    "        return docs\n",
    "    \n",
    "\n",
    "retriever = CustomMultiVectorRetriever(vectorstore=vectorstore, docstore=docstore) # need both vectorstore (for chuncks) and docstore (for parent docs)\n",
    "retriever.invoke(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0b0ea",
   "metadata": {},
   "source": [
    "Differences between \"CharacterTextSplitter + Simple Retriever\" vs \"MultiVectorRetriever\":\n",
    "\n",
    "1. Document Storage & Retrieval Granularity\n",
    "- Simple Retriever:\n",
    "  - Each chunk from CharacterTextSplitter is stored as a separate document in the vector store.\n",
    "  - Retrieval returns the individual matching chunk(s) only.\n",
    "- MultiVectorRetriever:\n",
    "  - Each chunk is stored in the vector store with a parent document ID.\n",
    "  - The full parent document is also stored in a docstore.\n",
    "  - Retrieval returns the **whole parent document**, not just the matching chunk.\n",
    "\n",
    "2. Output Context\n",
    "- Simple Retriever:\n",
    "  - User receives only the small snippet/chunk that matched.\n",
    "  - No direct information about the context or full document.\n",
    "- MultiVectorRetriever:\n",
    "  - User receives the entire parent document.\n",
    "  - Can also receive info about which chunk(s) triggered the match (in metadata).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c5cbc",
   "metadata": {},
   "source": [
    "### EnsembleRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3637986",
   "metadata": {},
   "source": [
    "The EnsembleRetriever is a tool in LangChain that lets you combine results from multiple retrievers (search algorithms).\n",
    "This is useful because different retrievers have different strengths:\n",
    "\n",
    "Sparse retrievers (like BM25) are good at keyword matching.\n",
    "\n",
    "Dense retrievers (like those using embeddings) are good at understanding meaning.\n",
    "\n",
    "Ensembling (combining) them gives you better, more robust results—this is often called hybrid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import SimpleRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"Banana is yellow\",\n",
    "    \"Apple is red\",\n",
    "    \"Grapes are green\",\n",
    "    \"Banana and apple are fruits\",\n",
    "]\n",
    "\n",
    "# Create a FAISS vector store with OpenAI embeddings\n",
    "embedding_fn = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(docs, embedding_fn)\n",
    "\n",
    "# Create BM25Retriever (uses the same docs, but doesn't use embeddings)\n",
    "bm25_retriever = BM25Retriever.from_texts(docs)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "# Create SimpleRetriever (wraps the FAISS vectorstore)\n",
    "simple_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Ensemble the two retrievers (equal weight)\n",
    "ensemble = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, simple_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Run a query\n",
    "results = ensemble.invoke(\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3ba06",
   "metadata": {},
   "source": [
    "### Long Context Reorder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7544c89",
   "metadata": {},
   "source": [
    "Usually queries against vector stores will typically return documents in descending order of relevance (e.g., as measured by cosine similarity of embeddings) and they are fed to llm in that particular sequence. \n",
    "But, LLM models are liable to miss relevant information in the middle of long contexts.\n",
    "\n",
    "To mitigate the \"lost in the middle\" effect, you can re-order documents after retrieval such that the most relevant documents are positioned at extrema (e.g., the first and last pieces of context), and the least relevant documents are positioned in the middle. In some cases this can help surface the most relevant information to LLMs.\n",
    "\n",
    "\n",
    "The LongContextReorder document transformer implements this re-ordering procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b608cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Get embeddings.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "texts = [\n",
    "    \"Basquetball is a great sport.\",\n",
    "    \"Fly me to the moon is one of my favourite songs.\",\n",
    "    \"The Celtics are my favourite team.\",\n",
    "    \"This is a document about the Boston Celtics\",\n",
    "    \"I simply love going to the movies\",\n",
    "    \"The Boston Celtics won the game by 20 points\",\n",
    "    \"This is just a random text.\",\n",
    "    \"Elden Ring is one of the best games in the last 15 years.\",\n",
    "    \"L. Kornet is one of the best Celtics players.\",\n",
    "    \"Larry Bird was an iconic NBA player.\",\n",
    "]\n",
    "\n",
    "# Create a retriever\n",
    "retriever = InMemoryVectorStore.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"What can you tell me about the Celtics?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.invoke(query)\n",
    "for doc in docs:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "# Reorder the documents: Less relevant document will be at the middle of the list and more relevant elements at beginning / end.\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# Confirm that the 4 relevant documents are at beginning and end.\n",
    "for doc in reordered_docs:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2dabe2",
   "metadata": {},
   "source": [
    "### ParentVectorRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa085ef",
   "metadata": {},
   "source": [
    "Sometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split each larger chunk it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents). \n",
    "\n",
    "As retriever is just a wrapper around vector store, you can add data after you define retriever.\n",
    "\n",
    "If you don't provide \"parent_splitter\" arg during instatiating retriever, then you can just retrieve the whole parent document rather than larger chunk of parent document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7653a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Two-level chunking with ParentDocumentRetriever in LangChain\n",
    "\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. Example raw document\n",
    "raw_docs = [\n",
    "    Document(page_content=\"\"\"\n",
    "    LangChain is a framework for developing applications powered by language models.\n",
    "    It enables the composition of LLMs with external data, APIs, and computation.\n",
    "    LangChain supports retrieval-augmented generation (RAG) pipelines.\n",
    "    This document describes chunking strategies for efficient retrieval.\n",
    "    Splitting documents helps balance retrieval accuracy and context size.\n",
    "    \"\"\"),\n",
    "    Document(page_content=\"\"\"\n",
    "    ParentDocumentRetriever enables two-level chunking.\n",
    "    First, split documents into large parent chunks (e.g., 200 chars).\n",
    "    Then, split parents into smaller child chunks (e.g., 80 chars).\n",
    "    Index only the child chunks for retrieval.\n",
    "    Upon retrieval, return the parent chunk for additional context.\n",
    "    \"\"\")\n",
    "]\n",
    "\n",
    "# 2. Define chunkers\n",
    "# Parent splitter: large chunks (~200 chars)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "# Child splitter: smaller chunks (~80 chars)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=80, chunk_overlap=0)\n",
    "\n",
    "# 3. Embedding function and vectorstore for child chunks\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS(embedding_function=embedding)\n",
    "\n",
    "# 4. Create ParentDocumentRetriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=None,  # Uses in-memory docstore by default\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "# 5. Add the documents (handles two-level chunking internally)\n",
    "retriever.add_documents(raw_docs)\n",
    "\n",
    "# 6. Query with a relevant term\n",
    "results = retriever.invoke(\"retrieval-augmented generation\")\n",
    "\n",
    "print(\"Retrieved parent chunk(s):\")\n",
    "for doc in results:\n",
    "    print(\"=\"*20)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a161d0",
   "metadata": {},
   "source": [
    "### Time Weighted Vector Store Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea8858",
   "metadata": {},
   "source": [
    "In many applications (like chatbots, personal assistants, or memory-augmented LLMs), you want your system to:\n",
    "\n",
    "1. Recall relevant information (semantic similarity)\n",
    "2. Prefer information that is recent or frequently used (recency/freshness)\n",
    "\n",
    "Problem:\n",
    "If you only retrieve by semantic similarity, you might get outdated or rarely-used information—even if something more recent is just as relevant!\n",
    "\n",
    "Time-weighted retrievers solve this by boosting the importance of recent (or recently accessed) information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fe50d",
   "metadata": {},
   "source": [
    "Each document gets a score based on:\n",
    "\n",
    "1. semantic_similarity (how much the content matches your query)\n",
    "2. recency_score (how recent or “fresh” the document is)\n",
    "\n",
    "score = semantic_similarity + (1.0 - decay_rate) ** hours_passed\n",
    "\n",
    "decay_rate: how quickly recency fades (0 = never fades, 1 = always faded)\n",
    "\n",
    "hours_passed: time since the document was last accessed (not just created!)\n",
    "\n",
    "if decay rate is close to 0, means old info never fades. and (1.0 - decay_rate) ** hours_passed = 1 and score ~ semantic_score.\n",
    "\n",
    "if decay rate is close to 1, means old info fades very fast. and (1.0 - decay_rate) ** hours_passed = 0 and score ~ recency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import faiss\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. Set up embeddings and vector store\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embedding_size = 1536\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n",
    "\n",
    "# 2. Create the time-weighted retriever\n",
    "retriever = TimeWeightedVectorStoreRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    decay_rate=0.05,  # Try 0.000...1 for persistent, 0.05 for moderate decay, or 0.99 for fast decay\n",
    "    k=1\n",
    ")\n",
    "\n",
    "# 3. Add documents (one 'last_accessed_at' set to yesterday)\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "retriever.add_documents([\n",
    "    Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday}),\n",
    "    Document(page_content=\"hello foo\")  # 'last_accessed_at' is now\n",
    "])\n",
    "\n",
    "# 4. Retrieve for a query\n",
    "results = retriever.invoke(\"hello world\")\n",
    "print(\"Most relevant & fresh result:\", results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9f489",
   "metadata": {},
   "source": [
    "When a query comes in, the retriever:\n",
    "\n",
    "1. Looks at each document's \"last_accessed_at\" timestamp.\n",
    "2. Calculates how many hours (or other time units) have passed since it was last accessed.\n",
    "3. Applies the time decay formula to adjust the score.\n",
    "4. Returns the most relevant and “fresh” document(s)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_langchain_2-zo9tOhzO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
